---
title: "AlunosUFCGRound"
author: "Rodolfo Viana"
date: "22-06-2015"
output: html_document
---

Durante os últimos anos a Universidade Federal de Campina Grande observa um número alto de evasão por parte dos alunos. Tentando entender os motivos dessa evasão analisamos uma amostra contendo dados importante. A nossa amostra tem os seguintes atributos:

1. MATRICULA: identificador do aluno
2. PERIODO: identificador do período letivo da universidade (ano.semestre)
3. COD_CURSO: identificador do curso
4. CURSO: nome do curso. Cada curso tem seu COD_CURSO
5. CODIGO: identificador da disciplina que o aluno cursou no periodo
6. DISCIPLINA: nome da disciplina referente que o aluno cursou no periodo.  
7. CODIGO: Cada disciplina tem seu
8. CREDITOS: numero de créditos referente a disciplina
9. DEPARTAMENTO: departamento que ofertou a disciplina
10. MEDIA: média do aluno na disciplina (0 a 10). Alunos reprovados por falta numa disciplina recebem 0 e alunos que trancaram a disciplina recebem NA.
11. STATUS: Aprovado, Reprovado Por Falta, Trancado ou Reprovado. Se refere ao estado final do aluno na disciplina
12. PERIODO_INGRESSO: período letivo da universidade em que o aluno ingressou no curso.
13. PERIODO_RELATIVO: número de períodos que o aluno está matriculado na universidade. "1" refere-se ao aluno em seu primeiro periodo, "5" refere-se ao aluno no quinto período.
14. COD_EVASAO: identificador de evasão do aluno. "0" significa que o aluno continuou ativo na universidade no período seguinte e "1" significa que o aluno desistiu do curso nesse período e não voltou a se matricular no seguinte. 

O nosso objetivo é construir um modelo de classificação que nos diga se o aluno irá evadir ou não. Vamos classificar para os alunos que tem período relativo 5. 

```{r, warning=FALSE, message=FALSE}
library(dplyr)

arquivo <- read.csv("~/Projetos/DataAnalysis/Assignment5/training_evasao_sem_acento.csv")

#Transformação para factor
arquivo$COD_EVASAO <- as.factor(arquivo$COD_EVASAO)
arquivo$COD_CURSO <- as.factor(arquivo$COD_CURSO)
arquivo$CODIGO <- as.factor(arquivo$CODIGO)
arquivo$CREDITOS <- as.factor(arquivo$CREDITOS)
arquivo$MEDIA[is.na(arquivo$MEDIA)] <- -1
```

Antes de criar o modelo é importante dividir o arquivo original em treino e teste (75% treinamento, 25% teste), para assim verificar o F-measure e saber se um modelo criado é melhor do que o modelo anterior. 

```{r}
#Primeiro periodo
set.seed(12345)
arquivo <- filter(arquivo, PERIODO_RELATIVO == 5)
arquivo <- arquivo[order(runif(nrow(arquivo))), ]

#Divisao de treino e teste
treino <- arquivo[1:round(0.75*nrow(arquivo)), ]
test <- arquivo[round(0.75*nrow(arquivo)):nrow(arquivo), ]
```

Podemos notar que a proporção entre evasão e não evasão se manteve parecida após a divisão de treino e teste. 

```{r}
prop.table(table(arquivo$COD_EVASAO))
prop.table(table(treino$COD_EVASAO))
prop.table(table(test$COD_EVASAO))
```

Inicialmente vamos criar um modelo considerando apenas o código do curso, código da disciplina cursada, departamento e a situação. 

```{r}
library("C50")

model <- C5.0(treino[,c(3,7,9,11)], treino$COD_EVASAO)
model
summary(model)

pred <- predict(model, test[,c(3,7,9,11)])

true_eva <- test$COD_EVASAO == 1
table(pred, true_eva)
```

Podemos notar que não criamos um bom classificador, pois ele classificou todas as saidas como sendo 0. Temos esse erro pois estamos com dados desbalanceados. No nosso arquivo de treino 94% dos dados são de não evasão e 6% dos dados são evasão. 

Por essa razão resolvemos por, nos dados de treino, balancear os dados. 

```{r}
treino_positivo = filter(treino, COD_EVASAO == 1)
treino_negativo = filter(treino, COD_EVASAO == 0)

treino_negativo <- treino_negativo[order(runif(nrow(treino_positivo))), ]

novo_treino <- rbind(treino_positivo,treino_negativo)
novo_treino <- novo_treino[order(runif(nrow(novo_treino))), ]
prop.table(table(novo_treino$COD_EVASAO))
```

Criamos novamente o modelo considerando apenas o código do curso, código da disciplina cursada, departamento e a situação. Dessa vez utilizando os dados de treino balanceados. 


```{r}
model <- C5.0(novo_treino[,c(3,7,9,11)], novo_treino$COD_EVASAO)
model
summary(model)

pred <- predict(model, test[,c(3,7,9,11)])

true_eva <- test$COD_EVASAO == 1
table(pred, true_eva)
```

Com esse modelo temos um F-measure igual a:

```{r}
precision <- 39/39+45
recall <- 39/(39+156)

fmeasure1 <- 2*precision*recall/(precision+recall)
fmeasure1
```

Com o intuito de verificar se essa forma de balanceamento é a ideal, testamos com uma proporção diferente. Dessa vez sendo 70% para não evasão e 30% para evasão

```{r}
treino_positivo = filter(treino, COD_EVASAO == 1)
treino_negativo = filter(treino, COD_EVASAO == 0)

treino_negativo <- treino_negativo[order(runif(nrow(treino_positivo)*2.4)), ]

novo_treino <- rbind(treino_positivo,treino_negativo)
novo_treino <- novo_treino[order(runif(nrow(novo_treino))), ]
prop.table(table(novo_treino$COD_EVASAO))

model <- C5.0(novo_treino[,c(3,7,9,11)], novo_treino$COD_EVASAO)
model
summary(model)

pred <- predict(model, test[,c(3,7,9,11)])

true_eva <- test$COD_EVASAO == 1
table(pred, true_eva)
```

É possivel notar que mesmo mudando o balanceamento para 70%,30% o valor do F-measure não mudou. Por essa razão escolhemos ficar com o balancemanto 50%,50%. 

```{r}
treino_positivo = filter(treino, COD_EVASAO == 1)
treino_negativo = filter(treino, COD_EVASAO == 0)

treino_negativo <- treino_negativo[order(runif(nrow(treino_positivo))), ]

novo_treino <- rbind(treino_positivo,treino_negativo)
novo_treino <- novo_treino[order(runif(nrow(novo_treino))), ]
prop.table(table(novo_treino$COD_EVASAO))
```

Em busca de melhorar o valor do F-measure criamos novos atributos:

1. Média geral do período do aluno
2. Quantidade de cadeiras aprovadas
3. Quantidade de reprovações
4. Quantidade de reprovações por falta

```{r}
#Criando novos atributos para o treino
treino_group <- group_by(novo_treino, MATRICULA)
media <- summarise(treino_group, mean(MEDIA))
names(media) <- c("MATRICULA", "MEDIATOTAL")

reprovacao <- summarise(group_by(filter(novo_treino, SITUACAO=="Reprovado"), MATRICULA), n())
names(reprovacao) <- c("MATRICULA", "REPROVACAO")

aprovado <- summarise(group_by(filter(novo_treino, SITUACAO=="Aprovado"), MATRICULA), n())
names(aprovado) <- c("MATRICULA", "APROVADO")

target <- c("Reprovado por Falta", NA)
reprovado_falta <- summarise(group_by(filter(novo_treino, SITUACAO %in% target), MATRICULA), n())
names(reprovado_falta) <- c("MATRICULA", "REPROVADOFALTA")

target <- c("Reprovado por Falta", "Reprovado", "Trancado")
n_aprovado <- summarise(group_by(filter(novo_treino, SITUACAO %in% target), MATRICULA), n())
names(n_aprovado) <- c("MATRICULA", "NAPROVADO")


#Merge dos novos atributos
novo_treino <- merge(novo_treino, media, by = "MATRICULA", all = TRUE)
novo_treino <- merge(novo_treino, reprovacao, by = "MATRICULA", all = TRUE)
novo_treino <- merge(novo_treino, aprovado, by = "MATRICULA", all = TRUE)
novo_treino <- merge(novo_treino, reprovado_falta, by = "MATRICULA", all = TRUE)
novo_treino <- merge(novo_treino, n_aprovado, by = "MATRICULA", all = TRUE)

#Transformando NA em 0
novo_treino[is.na(novo_treino)] <- 0

#Criando novos atributos para o teste
test_group <- group_by(test, MATRICULA)
media <- summarise(test_group, mean(MEDIA))
names(media) <- c("MATRICULA", "MEDIATOTAL")

reprovacao <- summarise(group_by(filter(test, SITUACAO=="Reprovado"), MATRICULA), n())
names(reprovacao) <- c("MATRICULA", "REPROVACAO")

aprovado <- summarise(group_by(filter(test, SITUACAO=="Aprovado"), MATRICULA), n())
names(aprovado) <- c("MATRICULA", "APROVADO")

target <- c("Reprovado por Falta", NA)
reprovado_falta <- summarise(group_by(filter(test, SITUACAO %in% target), MATRICULA), n())
names(reprovado_falta) <- c("MATRICULA", "REPROVADOFALTA")

target <- c("Reprovado por Falta", "Reprovado", "Trancado")
n_aprovado <- summarise(group_by(filter(test, SITUACAO %in% target), MATRICULA), n())
names(n_aprovado) <- c("MATRICULA", "NAPROVADO")


test <- merge(test, media, by = "MATRICULA", all = TRUE)
test <- merge(test, reprovacao, by = "MATRICULA", all = TRUE)
test <- merge(test, aprovado, by = "MATRICULA", all = TRUE)
test <- merge(test, reprovado_falta, by = "MATRICULA", all = TRUE)
test <- merge(test, n_aprovado, by = "MATRICULA", all = TRUE)

test[is.na(test)] <- 0
```

Agora com as 4 novas colunas criadas temos mais dados para analisar e ajudar no classificador. 

```{r}
model <- C5.0(novo_treino[,c(3,5,7,9,10,11,15,16,17,18,19)], novo_treino$COD_EVASAO)

model
summary(model)
pred <- predict(model, test[,c(3,5,7,9,10,11,15,16,17,18,19)])

true_eva <- test$COD_EVASAO == 1
table(pred, true_eva)
```

O valor do f-measure com os novos atributos é:

```{r}
precision <- 49/49+35
recall <- 49/(49+496)

fmeasure2 <- 2*precision*recall/(precision+recall)
fmeasure2
```

Bem inferior ao f-measure anterior.

Por essa razão resolvemos testar com o dataset sem está balanceado.

```{r}
#Criando novos atributos para o treino
treino_group <- group_by(treino, MATRICULA)
media <- summarise(treino_group, mean(MEDIA))
names(media) <- c("MATRICULA", "MEDIATOTAL")

reprovacao <- summarise(group_by(filter(treino, SITUACAO=="Reprovado"), MATRICULA), n())
names(reprovacao) <- c("MATRICULA", "REPROVACAO")

aprovado <- summarise(group_by(filter(treino, SITUACAO=="Aprovado"), MATRICULA), n())
names(aprovado) <- c("MATRICULA", "APROVADO")

target <- c("Reprovado por Falta", NA)
reprovado_falta <- summarise(group_by(filter(treino, SITUACAO %in% target), MATRICULA), n())
names(reprovado_falta) <- c("MATRICULA", "REPROVADOFALTA")

target <- c("Reprovado por Falta", "Reprovado", "Trancado")
n_aprovado <- summarise(group_by(filter(treino, SITUACAO %in% target), MATRICULA), n())
names(n_aprovado) <- c("MATRICULA", "NAPROVADO")


#Merge dos novos atributos
treino <- merge(treino, media, by = "MATRICULA", all = TRUE)
treino <- merge(treino, reprovacao, by = "MATRICULA", all = TRUE)
treino <- merge(treino, aprovado, by = "MATRICULA", all = TRUE)
treino <- merge(treino, reprovado_falta, by = "MATRICULA", all = TRUE)
treino <- merge(treino, n_aprovado, by = "MATRICULA", all = TRUE)

#Transformando NA em 0
treino[is.na(treino)] <- 0

model <- C5.0(treino[,c(3,5,7,9,10,11,15,16,17,18,19)], treino$COD_EVASAO)

model
summary(model)
pred <- predict(model, test[,c(3,5,7,9,10,11,15,16,17,18,19)])

true_eva <- test$COD_EVASAO == 1
table(pred, true_eva)

#calculo do f-measure
precision <- 32/32+52
recall <- 32/(32+57)

fmeasure3 <- 2*precision*recall/(precision+recall)
fmeasure3
```

É possivel notar que utilizando o arquivo sem está balanceado tivemos o melhor valor do F-measure até agora. 

Querendo aumentar o valor do F-measure utilizamos a estratégia de **random forest**. 

```{r}
model_florest <- C5.0(treino[,c(3,5,7,9,10,11,15,16,17,18,19)], treino$COD_EVASAO,trials = 10)
model_florest
summary(model_florest)

pred <- predict(model_florest, test[,c(3,5,7,9,10,11,15,16,17,18,19)])

true_eva <- test$COD_EVASAO == 1
table(pred, true_eva)

#calculo do f-measure
precision <- 17/17+67
recall <- 17/(17+27)

fmeasure4 <- 2*precision*recall/(precision+recall)
fmeasure4
```

É possivel notar que esse f-measure foi bem melhor do que os anteriores. 

```{r}
modelo = c(1, 2, 3, 4) 
fmeasure = c(fmeasure1, fmeasure2, fmeasure3, fmeasure4) 
dataframe = data.frame(fmeasure, modelo)

library(ggplot2)

ggplot(dataframe, aes(modelo, fmeasure) ) + geom_line() + theme(panel.background=element_blank())
```

Porém ao longo do processo percebenos que a matrix de confusão da nossa versão final é pior da versão com sem a quantidade de aprovações e reprovações por falta. Observe:

```{r}
test_real <- read.csv("~/Projetos/DataAnalysis/Assignment5/test_second_round_kaggle_sem_acento.csv")

#Transformação para factor
test_real$COD_CURSO <- as.factor(test_real$COD_CURSO)
test_real$CODIGO <- as.factor(test_real$CODIGO)
test_real$CREDITOS <- as.factor(test_real$CREDITOS)
test_real$MEDIA[is.na(test_real$MEDIA)] <- -1

media <- summarise(group_by(test_real, MATRICULA), mean(MEDIA))
names(media) <- c("MATRICULA", "MEDIATOTAL")

reprovacao <- summarise(group_by(filter(test_real, SITUACAO=="Reprovado"), MATRICULA), n())
names(reprovacao) <- c("MATRICULA", "REPROVACAO")

aprovado <- summarise(group_by(filter(test_real, SITUACAO=="Aprovado"), MATRICULA), n())
names(aprovado) <- c("MATRICULA", "APROVADO")

target <- c("Reprovado por Falta", NA)
reprovado_falta <- summarise(group_by(filter(test_real, SITUACAO %in% target), MATRICULA), n())
names(reprovado_falta) <- c("MATRICULA", "REPROVADOFALTA")

target <- c("Reprovado por Falta", "Reprovado", "Trancado")
n_aprovado <- summarise(group_by(filter(test_real, SITUACAO %in% target), MATRICULA), n())
names(n_aprovado) <- c("MATRICULA", "NAPROVADO")


#Merge dos novos atributos
test_real <- merge(test_real, media, by = "MATRICULA", all = TRUE)
test_real <- merge(test_real, reprovacao, by = "MATRICULA", all = TRUE)
test_real <- merge(test_real, aprovado, by = "MATRICULA", all = TRUE)
test_real <- merge(test_real, reprovado_falta, by = "MATRICULA", all = TRUE)
test_real <- merge(test_real, n_aprovado, by = "MATRICULA", all = TRUE)

#Transformando NA em 0
test_real[is.na(test_real)] <- 0

pred <- predict(model_florest, test_real[,c(3,5,7,9,10,11,14,15,16,17,18)])
summary(pred)

test_real["COD_EVASAO"] <- pred

write.csv(select(test_real, ID, COD_EVASAO), file="~/Projetos/DataAnalysis/Assignment5/submissao/submissaoRound15.csv", row.names = FALSE)

```

Esse último modelo foi o escolhido para a submissão final no kaggle. 